{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German wings review challenge\n",
    "\n",
    "The goal of this notebook is to show based on the customer review data how to predict the target variable 'Recommended'.\n",
    "The data is found in a text file holding a set of reviews about the German wings airline (airline code ‘4U’).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import sys, os, re, csv, codecs, math\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
    "\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy import interp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from hyperopt import hp, tpe, STATUS_OK, Trials\n",
    "from hyperopt.fmin import fmin\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "#stop_words = nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_df_review(filename='4U_Reviews.txt', encoding = \"ISO-8859-1\"):\n",
    "    \"\"\"\n",
    "    convert text data in the txt file into a pandas dataframe\n",
    "    \"\"\"\n",
    "    #map the content to dictionary\n",
    "    dict_review = {'Title':[], \n",
    "                   'User Name':[], \n",
    "                   'Country':[], \n",
    "                   'Date':[],\n",
    "                   'Rating':[], \n",
    "                   'Review':[], \n",
    "                   'Type Of Traveller':[], \n",
    "                   'Cabin Flown':[], \n",
    "                   'Route':[], \n",
    "                   'Date Flown':[], \n",
    "                   'Seat Comfort':[], \n",
    "                   'Cabin Staff Service':[], \n",
    "                   'Ground Service':[], \n",
    "                   'Value For Money':[], \n",
    "                   'Recommended':[]\n",
    "                  }\n",
    "\n",
    "    other_info_names = ['Type Of Traveller', \n",
    "                        'Cabin Flown', \n",
    "                        'Route', \n",
    "                        'Date Flown', \n",
    "                        'Seat Comfort', \n",
    "                        'Cabin Staff Service', \n",
    "                        'Ground Service', \n",
    "                        'Value For Money', \n",
    "                        'Recommended'\n",
    "                       ]\n",
    "    \n",
    "    f = open(filename, \"r\", encoding = encoding)\n",
    "    \n",
    "    review_info_list = [] \n",
    "    \n",
    "    for line in f:\n",
    "        if line == '\\n':\n",
    "            #print(\"-----\")\n",
    "            #print(review_info_list)\n",
    "\n",
    "            # get Title, User Name, Country, Date, Rating, Review\n",
    "            for i, line in enumerate(review_info_list):\n",
    "                # get title\n",
    "                if i == 0:\n",
    "                    #print('title: {}'.format(line))\n",
    "                    dict_review['Title'].append(line)\n",
    "                # get user name, country, date\n",
    "                elif i == 1:\n",
    "                    user_name = ''\n",
    "                    country_name = ''\n",
    "                    date_str = ''\n",
    "                    \n",
    "                    pos_start = line.find('(')\n",
    "                    pos_end = line.find(')')\n",
    "                    if pos_start != -1:\n",
    "                        user_name = line[0:pos_start].strip()\n",
    "                        country_name = line[pos_start+1:pos_end].strip()\n",
    "                        date_str = line[pos_end+1:].strip()\n",
    "                    else:\n",
    "                        str_list = line.split()\n",
    "                        if len(str_list) == 5:\n",
    "                            user_name = str_list[0] + ' ' + str_list[1]\n",
    "                            country_name = ''\n",
    "                            date_str = str_list[-3].strip() + ' ' + str_list[-2].strip() + ' ' + str_list[-1].strip()\n",
    "                        \n",
    "                    #print('user name: {}'.format(user_name))\n",
    "                    #print('country name: {}'.format(country_name))\n",
    "                    #print('date: {}'.format(date_str))\n",
    "                    dict_review['User Name'].append(user_name)\n",
    "                    dict_review['Country'].append(country_name)\n",
    "                    dict_review['Date'].append(date_str)\n",
    "                # get rating\n",
    "                elif i == 2:\n",
    "                    if line.strip() == 'na':\n",
    "                        rating = np.nan\n",
    "                    else:\n",
    "                        rating = int(line.strip())\n",
    "                    #print('rating: {}'.format(rating))\n",
    "                    dict_review['Rating'].append(rating)\n",
    "                # get review\n",
    "                elif i == 3:\n",
    "                    #print('review')\n",
    "                    #print(line.strip())\n",
    "                    dict_review['Review'].append(line.strip())\n",
    "            \n",
    "            # get other information     \n",
    "            for info_name in other_info_names:\n",
    "                review_info = ''\n",
    "                for i, line in enumerate(review_info_list):\n",
    "                    pos = line.find(info_name)\n",
    "                    if pos != -1:\n",
    "                        review_info = line[len(info_name)+1:].strip()\n",
    "                        break\n",
    "                dict_review[info_name].append(review_info)\n",
    "\n",
    "            review_info_list = [] \n",
    "        else:\n",
    "            line = line.replace('\\\"', '')\n",
    "            #print(line)\n",
    "            review_info_list.append(line)\n",
    "\n",
    "    df_review = pd.DataFrame(dict_review)\n",
    "    \n",
    "    # get route to and route from   \n",
    "    def f_from(x):\n",
    "        if x:\n",
    "            return x.split()[0]\n",
    "        return ''\n",
    "    def f_to(x):\n",
    "        if x:\n",
    "            return x.split()[2]\n",
    "        return ''\n",
    "    df_review['Route From'] = df_review['Route'].apply(f_from)\n",
    "    df_review['Route To'] = df_review['Route'].apply(f_to)\n",
    "    \n",
    "    return df_review    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_review = get_df_review()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Analyse target 'Recommended' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['Recommended'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['Recommended'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['Recommended'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transfer string into integer\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "df_review['Recommended Label'] = lbl_enc.fit_transform(df_review['Recommended'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_review['Recommended Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review[df_review['Recommended Label'] == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review[df_review['Recommended Label'] == 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows the data of two classes is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analyse target 'Rating' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['Rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['Rating'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review[df_review['Rating'].notnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_review[df_review['Rating'].notnull()]['Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Analyse Missing Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df_review.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_review.isnull().sum()/df_review.count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "# missing_data.head(20)\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that 10% of the customer does not rate the flights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Show relationship between 'Recommended' and other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Relationship with numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_relation_numerical(df, var, target):\n",
    "    data = pd.concat([df[target], df[var]], axis=1)\n",
    "    data.plot.scatter(x=var, y=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = ['Rating']\n",
    "\n",
    "for num_var in num_vars:\n",
    "    show_relation_numerical(df_review, num_var, 'Recommended Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that 'Rating' is an important feature to distinguish the target variable 'Recommended'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Relationship with categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['Country', 'Type Of Traveller', 'Cabin Flown', 'Seat Comfort', 'Cabin Staff Service', 'Ground Service', 'Value For Money']\n",
    "                  \n",
    "# https://stackoverflow.com/questions/28418988/how-to-make-a-histogram-from-a-list-of-strings-in-python\n",
    "def plot_strings(a, title=''):\n",
    "    letter_counts = Counter(a)\n",
    "    df = pd.DataFrame.from_dict(letter_counts, orient='index')\n",
    "    df.plot(kind='bar', title=title)\n",
    "\n",
    "    \n",
    "for cat_var in cat_vars:\n",
    "    df = df_review[df_review['Recommended Label'] == 1][[cat_var]]\n",
    "    plot_strings(df[cat_var].tolist(), title=cat_var+' (recommended = 1)')\n",
    "    \n",
    "    df = df_review[df_review['Recommended Label'] == 0][[cat_var]]\n",
    "    plot_strings(df[cat_var].tolist(), title=cat_var+' (recommended = 0)')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that there is a lot of missing data in the categorical features. The categorical features are not usefull to predict target variable 'Recommended'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Text analysis\n",
    "\n",
    "Reference:\n",
    "https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['Title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_strings(df_review['Title'], title='Title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that most of the reviews have the title 'Germanwings customer review'. Therefore, I will not use 'Title' as a feature to predict 'Recommended'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO: Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_review[\"Review\"] = df_review[\"Review\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommendation problem is formulated into a binary classification problem, i.e., classify the text data (review) into recommended and non-recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define evaluation metric\n",
    "Kaggle has specified multi-class log-loss as evaluation metric. \n",
    "This is implemented in the follow way \n",
    "taken from: \n",
    "https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py)\n",
    "https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot precision recall curve\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pr_curve(y_test, y_score, model_name, features):\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    \n",
    "    #print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "    \n",
    "    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "    \n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "    plt.title('2-class Precision-Recall curve: AP={0:0.2f}, model={1}, features={2}'.format(average_precision, \n",
    "                                                                                           model_name,\n",
    "                                                                                           features\n",
    "                                                                                          ))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot AUC curve\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "\n",
    "https://qiita.com/bmj0114/items/460424c110a8ce22d945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_auc_curve(y_test, y_score, model_name, features):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    score = roc_auc_score(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic: ROC_AUC_Score={0:0.2f}, model={1}, features={2}'.format(score, \n",
    "                                                                                                          model_name,\n",
    "                                                                                                          features\n",
    "                                                                                                         ))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df_review['Recommended Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(df_review['Review'].values, \n",
    "                                                  y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, \n",
    "                                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### TF-IDF\n",
    "The first model is a TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  \n",
    "                      max_features=None, \n",
    "                      strip_accents='unicode', \n",
    "                      analyzer='word',\n",
    "                      token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1, 3), \n",
    "                      use_idf=1,\n",
    "                      smooth_idf=1,\n",
    "                      sublinear_tf=1,\n",
    "                      stop_words = 'english',\n",
    "                     )\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "ypred = clf.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Logloss\n",
    "\n",
    "For a binary classification like our example, the typical loss function is the binary cross-entropy / log loss.\n",
    "\n",
    "$$H_p(q) = - \\frac{1}{N} \\sum_{i=1}^{N} y_i \\times log(p(y_i)) + (1-y_i) \\times log(1-p(y_i))$$\n",
    "\n",
    "Neural networks are trained using stochastic gradient descent and require that you choose a loss function when designing and configuring your model.\n",
    "\n",
    "Given input, the model is trying to make predictions that match the data distribution of the target variable. Under maximum likelihood, a loss function estimates how closely the distribution of predictions made by a model matches the distribution of target variables in the training data.\n",
    "\n",
    "**Cross-Entropy Loss** (or Log Loss)\n",
    "\n",
    "Cross-entropy loss is often simply referred to as “cross-entropy,” “logarithmic loss,” “logistic loss,” or “log loss” for short.\n",
    "\n",
    "Each predicted probability is compared to the actual class output value (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected value. The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score for a large difference (0.9 or 1.0).\n",
    "\n",
    "Cross-entropy loss is minimized, where smaller values represent a better model than larger values. A model that predicts perfect probabilities has a cross entropy or log loss of 0.0.\n",
    "\n",
    "Cross-entropy for a binary or two class prediction problem is actually calculated as the average cross entropy across all examples.\n",
    "\n",
    "Given a framework of maximum likelihood, we know that we want to use a cross-entropy or mean squared error loss function under stochastic gradient descent.\n",
    "\n",
    "Nevertheless, we may or may not want to report the performance of the model using the loss function.\n",
    "\n",
    "For example, logarithmic loss is challenging to interpret, especially for non-machine learning practitioner stakeholders. The same can be said for the mean squared error. Instead, it may be more important to report the accuracy and root mean squared error for models used for classification and regression respectively.\n",
    "\n",
    "It may also be desirable to choose models based on these metrics instead of loss. This is an important consideration, as the model with the minimum loss may not be the model with best metric that is important to project stakeholders.\n",
    "\n",
    "A good division to consider is to use the loss to evaluate and diagnose how well the model is learning. This includes all of the considerations of the optimization process, such as overfitting, underfitting, and convergence. An alternate metric can then be chosen that has meaning to the project stakeholders to both evaluate model performance and perform model selection.\n",
    "\n",
    "The same metric can be used for both concerns but it is more likely that the concerns of the optimization process will differ from the goals of the project and different scores will be required. Nevertheless, it is often the case that improving the loss improves or, at worst, has no effect on the metric of interest.\n",
    "\n",
    "\n",
    "**Reference**\n",
    "\n",
    "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "https://medium.com/30-days-of-machine-learning/day-5-entropy-relative-entropy-and-cross-entropy-8369d67cc180\n",
    "\n",
    "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Precision Recall curve\n",
    "\n",
    "    - Precision-Recall curves summarize the trade-off between the **true positive rate** and the **positive predictive value** for a predictive model using different probability thresholds.\n",
    "    - Precision-Recall curves are appropriate for imbalanced datasets, whereas ROC curves are appropriate when the observations are balanced between each class.\n",
    "\n",
    "**Precision** is a ratio of the number of true positives divided by the sum of the true positives and false positives. It describes how good a model is at predicting the positive class. Precision is referred to as the positive predictive value.\n",
    "\n",
    "$${Precision} = \\frac{True Positives} {(True Positives + False Positives)}$$\n",
    "\n",
    "**Recall** is calculated as the ratio of the number of true positives divided by the sum of the true positives and the false negatives. Recall is the same as sensitivity.\n",
    "\n",
    "$${Recall} = \\frac{True Positives} {(True Positives + False Negatives)}$$\n",
    "\n",
    "Reviewing both precision and recall is useful in cases where there is an imbalance in the observations between the two classes. Specifically, there are many examples of no event (class 0) and only a few examples of an event (class 1).\n",
    "\n",
    "The reason for this is that typically the large number of class 0 examples means we are less interested in the skill of the model at predicting class 0 correctly, e.g. high true negatives.\n",
    "\n",
    "There are also composite scores that attempt to summarize the precision and recall; three examples include:\n",
    "\n",
    "- **F score** or **F1 score**: that calculates the harmonic mean of the precision and recall (harmonic mean because the precision and recall are ratios).\n",
    "\n",
    "- **Average precision**: that summarizes the weighted increase in precision with each change in recall for the thresholds in the precision-recall curve.\n",
    "\n",
    "- **Area Under Curve**: like the AUC, summarizes the integral or an approximation of the area under the precision-recall curve.\n",
    "\n",
    "In terms of model selection, F1 summarizes model skill for a specific probability threshold, whereas average precision and area under curve summarize the skill of a model across thresholds, like ROC AUC.\n",
    "\n",
    "This makes precision-recall and a plot of precision vs. recall and summary measures useful tools for binary classification problems that have an imbalance in the observations for each class.\n",
    "\n",
    "The above explaination of Precision and Recall is taken from\n",
    "\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='logistic regression', features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### ROC curve\n",
    "\n",
    "    - ROC Curves summarize the trade-off between the **true positive rate** and **false positive rate** for a predictive model using different probability thresholds.\n",
    "    \n",
    "    - ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\n",
    "    \n",
    "$${True Positive Rate} = \\frac{True Positives} {(True Positives + False Negatives)}$$\n",
    "The true positive rate is also referred to as sensitivity.\n",
    "\n",
    "$${False Positive Rate} = \\frac{False Positives} {(False Positives + True Negatives)}$$\n",
    "\n",
    "The false positive rate is also referred to as the inverted specificity where specificity is the total number of true negatives divided by the sum of the number of true negatives and false positives.\n",
    "\n",
    "$${Specificity} = \\frac{True Negatives} {(True Negatives + False Positives)}$$\n",
    "\n",
    "$${False Positive Rate} = 1 - {Specificity}$$\n",
    "\n",
    "The ROC curve is a useful tool for a few reasons:\n",
    "\n",
    "- The curves of different models can be compared directly in general or for different thresholds.\n",
    "- The area under the curve (AUC) can be used as a summary of the model skill.\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='logistic regression',  features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_error_pred(yvalid, ypred):\n",
    "    \"\"\"\n",
    "    Generate a dataframe contains the missclassified data\n",
    "    \"\"\"\n",
    "    index_error = yvalid != ypred\n",
    "    x_error = xvalid[index_error]\n",
    "    ytrue_error = yvalid[index_error].values\n",
    "    ypred_error = ypred[index_error]\n",
    "    \n",
    "    df_error = pd.DataFrame({'x':x_error, 'y_true':ytrue_error, 'y_pred':ypred_error})\n",
    "    \n",
    "    return df_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_error = get_error_pred(yvalid, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_error.iterrows():\n",
    "    print('index: {}'.format(index))\n",
    "    print(row['x'])\n",
    "    print('true label: {}'.format(row['y_true']))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review 2 is not clear for me it was positive or negative.\n",
    "\n",
    "Reviews 4, 5 has some pros and cons.\n",
    "\n",
    "Reviews 3, 6 could be neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Count Features\n",
    "\n",
    "Instead of using TF-IDF, we can also use word counts as features. This can be done easily using CountVectorizer from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',\n",
    "                      token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1, 3), \n",
    "                      stop_words = 'english'\n",
    "                     )\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "ypred = clf.predict(xvalid_ctv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='logistic regression', features='count features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='logistic regression',  features='count features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "ypred = clf.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='Naive Bayes', features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='Naive Bayes',  features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that Navis bayes classifier with count features achieves better ROC AUC score (0.87) compare to other settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple Naive Bayes on Counts\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "ypred = clf.predict(xvalid_ctv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='Naive Bayes', features='count features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='Naive Bayes',  features='count features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "ypred = clf.predict(xvalid_tfv.tocsc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='XGBoost', features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='XGBoost',  features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on count features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "ypred = clf.predict(xvalid_ctv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='XGBoost', features='count features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='XGBoost',  features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that with xgboost on the tfidf, we achieve better roc-auc score compared to count features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_xgb = xgb.XGBClassifier()\n",
    "\n",
    "param_dist = {'learning_rate': [0.01, 0.05, 0.07], \n",
    "              'max_depth': [4, 6],\n",
    "              'n_estimators': [100, 200, 500],\n",
    "              #'min_child_weight': [2, 4],\n",
    "              #'silent': [1],\n",
    "              #'subsample': [0.7],\n",
    "              #'colsample_bytree': [0.7],\n",
    "              #'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "             }\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "random_search = RandomizedSearchCV(estimator = regr_xgb, \n",
    "                                   param_distributions = param_dist, \n",
    "                                   n_jobs = 4,\n",
    "                                   n_iter = 2,\n",
    "                                   cv = 3,\n",
    "                                   verbose=2,\n",
    "                                   scoring=mll_scorer,\n",
    "                                  )\n",
    "\n",
    "random_search.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "\n",
    "print('xgboost (hyperparameter tuning with random search)')\n",
    "print(random_search.best_score_)\n",
    "print(random_search.best_params_)\n",
    "\n",
    "predictions = random_search.predict_proba(xvalid_tfv.tocsc())\n",
    "ypred = random_search.predict(xvalid_tfv.tocsc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='XGBoost (RandomSearch)', features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='XGBoost (RandomSearch)',  features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that random search hyperparameter tuning does not improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Word Embeddings \n",
    "\n",
    "Download the GloVe vectors from here \n",
    "http://www-nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "\n",
    "\n",
    "**Reference**\n",
    "\n",
    "- Word embedding\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=5PL0TmQhItY\n",
    "\n",
    "\n",
    "\n",
    "- Glove embedding\n",
    "\n",
    "https://www.kaggle.com/stacykurnikova/using-glove-embedding\n",
    "\n",
    "https://www.quora.com/How-is-GloVe-different-from-word2vec\n",
    "\n",
    "\n",
    "- Fast word embedding\n",
    "\n",
    "https://www.kaggle.com/vsmolyakov/keras-cnn-with-fasttext-embeddings\n",
    "\n",
    "https://spenai.org/bravepineapple/faster_em/\n",
    "\n",
    "\n",
    "- NLP\n",
    "\n",
    "https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-069\n",
    "\n",
    "https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    #stop_words = nltk.download('stopwords')\n",
    "    #words = str(s).lower().decode('utf-8')\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "ypred = clf.predict(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='XGBoost', features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='XGBoost',  features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that using xgboost with glove embedding features does not improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvalid_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=xtrain_glove.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=10, verbose=1, \n",
    "          #validation_data=(xvalid_glove_scl, yvalid_enc)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(xvalid_glove_scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = predictions.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='deep learning', features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='deep learning',  features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# keep on tuning the parameters of the neural network, add more layers, increase dropout to get better results. \n",
    "# To move further, i.e. with LSTMs we need to tokenize the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, xtrain_glove.shape[1]))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     xtrain_glove.shape[1],\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     50,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     50,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     50,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Reference\n",
    "\n",
    "**EDA**\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "https://medium.com/30-days-of-machine-learning/day-5-entropy-relative-entropy-and-cross-entropy-8369d67cc180\n",
    "\n",
    "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\n",
    "\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "\n",
    "\n",
    "**NLP**\n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/kernels\n",
    "\n",
    "https://www.kaggle.com/c/spooky-author-identification/kernels\n",
    "\n",
    "https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n",
    "\n",
    "https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing\n",
    "\n",
    "https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline\n",
    "\n",
    "\n",
    "\n",
    "https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams\n",
    "\n",
    "\n",
    "**General Data Science**\n",
    "\n",
    "https://www.kaggle.com/kernels?sortBy=voteCount&group=everyone&pageSize=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
