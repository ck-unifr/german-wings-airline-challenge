{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German wings review challenge\n",
    "\n",
    "The goal of this notebook is to show that based on the customer review data how to predict the target variable 'Recommended'.\n",
    "The data is found in a text file holding a set of reviews about the German wings airline (airline code ‘4U’).\n",
    "\n",
    "The notebook covers the following topics:\n",
    "\n",
    "- #### Data loading\n",
    "    - loading the customer review data from txt to dataframe\n",
    "\n",
    "- #### EDA\n",
    "    - plotting distribution of the variables\n",
    "    - plotting histograms\n",
    "    - showing relationship between target variables to other variables\n",
    "    - text analysis\n",
    "        - plotting word frequencies\n",
    "        - topic modeling with LDA\n",
    "    \n",
    "- #### Feature engineering\n",
    "    - tfidf\n",
    "    - count features\n",
    "    - word embedding\n",
    "        - glove\n",
    "    \n",
    "- #### Evaluation\n",
    "    - logloss\n",
    "    - roc curve\n",
    "    - precision recall curve\n",
    "\n",
    "- #### Machine learning models\n",
    "    - logistic regression\n",
    "    - navie bayes\n",
    "    - gradient boost machine\n",
    "        - xgboost\n",
    "    - deep learning\n",
    "        - lstm\n",
    "        - gru\n",
    "\n",
    "- #### Error analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import sys, os, re, csv, codecs, math, logging, base64\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
    "\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "\n",
    "from scipy import stats, interp\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.misc import imread\n",
    "\n",
    "# Plotly imports\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, RobustScaler, label_binarize\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_absolute_error, mean_squared_error, confusion_matrix\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from hyperopt import hp, tpe, STATUS_OK, Trials\n",
    "from hyperopt.fmin import fmin\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#stop_words = nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: If I have time, I will refactor this function.\n",
    "\n",
    "def get_df_review(filename='4U_Reviews.txt', encoding = \"ISO-8859-1\"):\n",
    "    \"\"\"\n",
    "    Load data in the txt file into a pandas dataframe.\n",
    "    \n",
    "    - Loop the text lines of the file.\n",
    "    - If the text line is not an empty line, then add the text line to the list.\n",
    "    - Else I process the date in the list and save the processed data into the dictionary. After that, I clean the list.\n",
    "    - At the end of the text, I convert the dictionary into a pandas dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    #map the content to dictionary\n",
    "    dict_review = {'Title':[], \n",
    "                   'User Name':[], \n",
    "                   'Country':[], \n",
    "                   'Date':[],\n",
    "                   'Rating':[], \n",
    "                   'Review':[], \n",
    "                   'Type Of Traveller':[], \n",
    "                   'Cabin Flown':[], \n",
    "                   'Route':[], \n",
    "                   'Date Flown':[], \n",
    "                   'Seat Comfort':[], \n",
    "                   'Cabin Staff Service':[], \n",
    "                   'Ground Service':[], \n",
    "                   'Value For Money':[], \n",
    "                   'Recommended':[]\n",
    "                  }\n",
    "\n",
    "    other_info_names = ['Type Of Traveller', \n",
    "                        'Cabin Flown', \n",
    "                        'Route', \n",
    "                        'Date Flown', \n",
    "                        'Seat Comfort', \n",
    "                        'Cabin Staff Service', \n",
    "                        'Ground Service', \n",
    "                        'Value For Money', \n",
    "                        'Recommended'\n",
    "                       ]\n",
    "    \n",
    "    f = open(filename, \"r\", encoding = encoding)\n",
    "    \n",
    "    # The list is used to save temporal review data. \n",
    "    review_info_list = [] \n",
    "    for line in f:\n",
    "        if line == '\\n':\n",
    "            #print(\"-----\")\n",
    "            #print(review_info_list)\n",
    "\n",
    "            # get Title, User Name, Country, Date, Rating, Review\n",
    "            for i, line in enumerate(review_info_list):\n",
    "                # get title\n",
    "                if i == 0:\n",
    "                    #print('title: {}'.format(line))\n",
    "                    dict_review['Title'].append(line)\n",
    "                # get user name, country, date\n",
    "                elif i == 1:\n",
    "                    user_name = ''\n",
    "                    country_name = ''\n",
    "                    date_str = ''\n",
    "                    \n",
    "                    pos_start = line.find('(')\n",
    "                    pos_end = line.find(')')\n",
    "                    if pos_start != -1:\n",
    "                        user_name = line[0:pos_start].strip()\n",
    "                        country_name = line[pos_start+1:pos_end].strip()\n",
    "                        date_str = line[pos_end+1:].strip()\n",
    "                    else:\n",
    "                        str_list = line.split()\n",
    "                        if len(str_list) == 5:\n",
    "                            user_name = str_list[0] + ' ' + str_list[1]\n",
    "                            country_name = ''\n",
    "                            date_str = str_list[-3].strip() + ' ' + str_list[-2].strip() + ' ' + str_list[-1].strip()\n",
    "                        \n",
    "                    #print('user name: {}'.format(user_name))\n",
    "                    #print('country name: {}'.format(country_name))\n",
    "                    #print('date: {}'.format(date_str))\n",
    "                    dict_review['User Name'].append(user_name)\n",
    "                    dict_review['Country'].append(country_name)\n",
    "                    dict_review['Date'].append(date_str)\n",
    "                # get rating\n",
    "                elif i == 2:\n",
    "                    if line.strip() == 'na':\n",
    "                        rating = np.nan\n",
    "                    else:\n",
    "                        rating = int(line.strip())\n",
    "                    #print('rating: {}'.format(rating))\n",
    "                    dict_review['Rating'].append(rating)\n",
    "                # get review\n",
    "                elif i == 3:\n",
    "                    #print('review')\n",
    "                    #print(line.strip())\n",
    "                    dict_review['Review'].append(line.strip())\n",
    "            \n",
    "            # get other information     \n",
    "            for info_name in other_info_names:\n",
    "                review_info = ''\n",
    "                for i, line in enumerate(review_info_list):\n",
    "                    pos = line.find(info_name)\n",
    "                    if pos != -1:\n",
    "                        review_info = line[len(info_name)+1:].strip()\n",
    "                        break\n",
    "                dict_review[info_name].append(review_info)\n",
    "\n",
    "            review_info_list = [] \n",
    "        else:\n",
    "            line = line.replace('\\\"', '')\n",
    "            #print(line)\n",
    "            review_info_list.append(line)\n",
    "\n",
    "    df_review = pd.DataFrame(dict_review)\n",
    "    \n",
    "    # get route to and route from   \n",
    "    def f_from(x):\n",
    "        if x:\n",
    "            return x.split()[0]\n",
    "        return ''\n",
    "    def f_to(x):\n",
    "        if x:\n",
    "            return x.split()[2]\n",
    "        return ''\n",
    "    df_review['Route From'] = df_review['Route'].apply(f_from)\n",
    "    df_review['Route To'] = df_review['Route'].apply(f_to)\n",
    "    \n",
    "    return df_review    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_review = get_df_review()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Analyse target 'Recommended' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['Recommended'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['Recommended'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['Recommended'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transfer string into integer\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "df_review['Recommended Label'] = lbl_enc.fit_transform(df_review['Recommended'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_review['Recommended Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review[df_review['Recommended Label'] == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review[df_review['Recommended Label'] == 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows the data of two classes is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analyse target 'Rating' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['Rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['Rating'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review[df_review['Rating'].notnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_review[df_review['Rating'].notnull()]['Rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Analyse Missing Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df_review.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_review.isnull().sum()/df_review.count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "# missing_data.head(20)\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that 10% of the customer does not rate the flights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Show relationship between 'Recommended' and other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Relationship with numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_relation_numerical(df, var, target):\n",
    "    data = pd.concat([df[target], df[var]], axis=1)\n",
    "    data.plot.scatter(x=var, y=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = ['Rating']\n",
    "\n",
    "for num_var in num_vars:\n",
    "    show_relation_numerical(df_review, num_var, 'Recommended Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that 'Rating' is an important feature to distinguish the target variable 'Recommended'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Relationship with categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['Country', 'Type Of Traveller', 'Cabin Flown', 'Seat Comfort', 'Cabin Staff Service', 'Ground Service', 'Value For Money']\n",
    "                  \n",
    "# https://stackoverflow.com/questions/28418988/how-to-make-a-histogram-from-a-list-of-strings-in-python\n",
    "def plot_strings(a, title=''):\n",
    "    letter_counts = Counter(a)\n",
    "    df = pd.DataFrame.from_dict(letter_counts, orient='index')\n",
    "    df.plot(kind='bar', title=title)\n",
    "\n",
    "    \n",
    "for cat_var in cat_vars:\n",
    "    df = df_review[df_review['Recommended Label'] == 1][[cat_var]]\n",
    "    plot_strings(df[cat_var].tolist(), title=cat_var+' (recommended = 1)')\n",
    "    \n",
    "    df = df_review[df_review['Recommended Label'] == 0][[cat_var]]\n",
    "    plot_strings(df[cat_var].tolist(), title=cat_var+' (recommended = 0)')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that there is a lot of missing data in the categorical features. The categorical features are not usefull to predict target variable 'Recommended'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Text analysis\n",
    "\n",
    "Reference\n",
    "\n",
    "https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n",
    "\n",
    "https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n",
    "\n",
    "https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['Title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_strings(df_review['Title'], title='Title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that most of the reviews have the title 'Germanwings customer review'. Therefore, I will not use 'Title' as a feature to predict 'Recommended'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_review[\"Review\"] = df_review[\"Review\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code is adapted from\n",
    "# https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n",
    "\n",
    "all_words = df_review[\"Review\"].str.split(expand=True).unstack().value_counts()\n",
    "data = [go.Bar(\n",
    "            x = all_words.index.values[2:50],\n",
    "            y = all_words.values[2:50],\n",
    "            marker= dict(colorscale='Jet',\n",
    "                         color = all_words.values[2:100]\n",
    "                        ),\n",
    "            text='Word counts'\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Top 50 (Uncleaned) Word frequencies in the dataset'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = df_review[df_review['Recommended']==\"yes\"][\"Review\"].str.split(expand=True).unstack().value_counts()\n",
    "data = [go.Bar(\n",
    "            x = all_words.index.values[2:50],\n",
    "            y = all_words.values[2:50],\n",
    "            marker= dict(colorscale='Jet',\n",
    "                         color = all_words.values[2:100]\n",
    "                        ),\n",
    "            text='Word counts'\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Top 50 (Uncleaned) Word frequencies in the positive reviews'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = df_review[df_review['Recommended']==\"no\"][\"Review\"].str.split(expand=True).unstack().value_counts()\n",
    "data = [go.Bar(\n",
    "            x = all_words.index.values[2:50],\n",
    "            y = all_words.values[2:50],\n",
    "            marker= dict(colorscale='Jet',\n",
    "                         color = all_words.values[2:100]\n",
    "                        ),\n",
    "            text='Word counts'\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Top 50 (Uncleaned) Word frequencies in the negative reviews'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_review = df_review[df_review['Recommended']==\"yes\"][\"Review\"].values\n",
    "neg_review = df_review[df_review['Recommended']==\"no\"][\"Review\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,13))\n",
    "wc = WordCloud(background_color=\"black\", \n",
    "               max_words=50, \n",
    "               #mask=hcmask1, \n",
    "               stopwords=STOPWORDS, \n",
    "               max_font_size= 40\n",
    "              )\n",
    "wc.generate(\" \".join(pos_review))\n",
    "\n",
    "plt.title(\"Positive Reviews\", fontsize=20)\n",
    "\n",
    "# plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\n",
    "\n",
    "plt.imshow(wc.recolor(colormap= 'Pastel2' , random_state=17), alpha=0.98)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,13))\n",
    "wc = WordCloud(background_color=\"black\", \n",
    "               max_words=50, \n",
    "               #mask=hcmask2, \n",
    "               stopwords=STOPWORDS, \n",
    "               max_font_size= 40\n",
    "              )\n",
    "wc.generate(\" \".join(neg_review))\n",
    "\n",
    "plt.title(\"Negative Reviews\", fontsize=20)\n",
    "\n",
    "# plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\n",
    "\n",
    "plt.imshow(wc.recolor(colormap= 'Pastel2' , random_state=17), alpha=0.98)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## 2.6 Topic Modeling \n",
    "\n",
    "\n",
    "One of the primary applications of natural language processing is to automatically extract what topics people are discussing from large volumes of text. Some examples of large text could be feeds from social media, customer reviews of hotels, movies, etc, user feedbacks, news stories, e-mails of customer complaints etc.\n",
    "\n",
    "Knowing what people are talking about and understanding their problems and opinions is highly valuable to businesses, administrators, political campaigns. And it’s really hard to manually read through such large volumes and compile the topics.\n",
    "\n",
    "Thus is required an automated algorithm that can read through the text documents and automatically output the topics discussed.\n",
    "\n",
    "\n",
    "\n",
    "**Reference**\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n",
    "\n",
    "https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 2.6.1 Stemming and Lemmatization\n",
    "\n",
    "The work at this stage attempts to reduce as many different variations of similar words into a single term ( different branches all reduced to single word stem). Therefore if we have \"running\", \"runs\" and \"run\", you would really want these three distinct words to collapse into just the word \"run\". (However of course you lose granularity of the past, present or future tense).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The stemmed form of running is: {}\".format(stemmer.stem(\"running\")))\n",
    "print(\"The stemmed form of runs is: {}\".format(stemmer.stem(\"runs\")))\n",
    "print(\"The stemmed form of run is: {}\".format(stemmer.stem(\"run\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn to another that we could use in lieu of stemming. \n",
    "This method is called lemmatization which aims to achieve the same effect as the former method. \n",
    "However unlike a stemmer, lemmatizing the dataset aims to reduce words based on an actual dictionary or vocabulary (the Lemma) and therefore will not chop off words into stemmed forms that do not carry any lexical meaning. \n",
    "\n",
    "Here we can utilize NLTK once again to initialize a lemmatizer (WordNet variant) and inspect how it collapses words as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "print(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "class LemmaCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Storing the entire training text in a list\n",
    "text = list(df_review['Review'].values)\n",
    "# Calling our overwritten Count vectorizer\n",
    "tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n",
    "                                     min_df=2,\n",
    "                                     stop_words='english',\n",
    "                                     decode_error='ignore')\n",
    "tf = tf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "count_vec = np.asarray(tf.sum(axis=0)).ravel()\n",
    "\n",
    "zipped = list(zip(feature_names, count_vec))\n",
    "\n",
    "x, y = (list(x) for x in zip(*sorted(zipped, key=lambda x: x[1], reverse=True)))\n",
    "\n",
    "# Now I want to extract out on the top 15 and bottom 15 words\n",
    "Y = np.concatenate([y[0:15], y[-16:-1]])\n",
    "X = np.concatenate([x[0:15], x[-16:-1]])\n",
    "\n",
    "# Plotting the Plot.ly plot for the Top 50 word frequencies\n",
    "data = [go.Bar(\n",
    "            x = x[0:50],\n",
    "            y = y[0:50],\n",
    "            marker= dict(colorscale='Jet',\n",
    "                         color = y[0:50]\n",
    "                        ),\n",
    "            text='Word counts'\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Top 50 Word frequencies after Preprocessing'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')\n",
    "\n",
    "# Plotting the Plot.ly plot for the Top 50 word frequencies\n",
    "data = [go.Bar(\n",
    "            x = x[-100:],\n",
    "            y = y[-100:],\n",
    "            marker= dict(colorscale='Portland',\n",
    "                         color = y[-100:]\n",
    "                        ),\n",
    "            text='Word counts'\n",
    "    )]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 2.6.2 LDA\n",
    "\n",
    "\n",
    "The LDA algorithm first models documents via a mixture model of topics. \n",
    "From these topics, words are then assigned weights based on the probability distribution of these topics. \n",
    "It is this probabilistic assignment over words that allow a user of LDA to say how likely a particular word falls into a topic. \n",
    "Subsequently from the collection of words assigned to a particular topic, are we thus able to gain an insight as to what that topic may actually represent from a lexical point of view.\n",
    "\n",
    "\n",
    "** Reference **\n",
    "\n",
    "https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation\n",
    "\n",
    "https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=11, max_iter=5,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define helper function to print top words\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #{}:\".format(index)\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n",
    "        print(message)\n",
    "        print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 40\n",
    "print(\"\\nTopics in LDA model: \")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_topic = lda.components_[0]\n",
    "second_topic = lda.components_[1]\n",
    "third_topic = lda.components_[2]\n",
    "fourth_topic = lda.components_[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_topic_words = [tf_feature_names[i] for i in first_topic.argsort()[:-50 - 1 :-1]]\n",
    "second_topic_words = [tf_feature_names[i] for i in second_topic.argsort()[:-50 - 1 :-1]]\n",
    "third_topic_words = [tf_feature_names[i] for i in third_topic.argsort()[:-50 - 1 :-1]]\n",
    "fourth_topic_words = [tf_feature_names[i] for i in fourth_topic.argsort()[:-50 - 1 :-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                       background_color='black',\n",
    "                       width=2500,\n",
    "                       height=1800\n",
    "                      ).generate(\" \".join(first_topic_words))\n",
    "plt.imshow(firstcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                       background_color='black',\n",
    "                       width=2500,\n",
    "                       height=1800\n",
    "                      ).generate(\" \".join(second_topic_words))\n",
    "plt.imshow(firstcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                       background_color='black',\n",
    "                       width=2500,\n",
    "                       height=1800\n",
    "                      ).generate(\" \".join(third_topic_words))\n",
    "plt.imshow(firstcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                       background_color='black',\n",
    "                       width=2500,\n",
    "                       height=1800\n",
    "                      ).generate(\" \".join(fourth_topic_words))\n",
    "plt.imshow(firstcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "\n",
    "visualizing the topics \n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommendation problem is formulated into a binary classification problem, i.e., classify the text data (review) into recommended and non-recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define evaluation metric\n",
    "Kaggle has specified multi-class log-loss as evaluation metric. \n",
    "This is implemented in the follow way \n",
    "taken from: \n",
    "https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py)\n",
    "https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot precision recall curve\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pr_curve(y_test, y_score, model_name, features):\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    \n",
    "    #print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "    \n",
    "    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "    \n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "    plt.title('2-class Precision-Recall curve: AP={0:0.2f}, model={1}, features={2}'.format(average_precision, \n",
    "                                                                                           model_name,\n",
    "                                                                                           features\n",
    "                                                                                          ))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot AUC curve\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "\n",
    "https://qiita.com/bmj0114/items/460424c110a8ce22d945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_auc_curve(y_test, y_score, model_name, features):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    score = roc_auc_score(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic: ROC_AUC_Score={0:0.2f}, model={1}, features={2}'.format(score, \n",
    "                                                                                                          model_name,\n",
    "                                                                                                          features\n",
    "                                                                                                         ))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df_review['Recommended Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(df_review['Review'].values, \n",
    "                                                  y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, \n",
    "                                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### TF-IDF\n",
    "The first model is a TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  \n",
    "                      max_features=None, \n",
    "                      strip_accents='unicode', \n",
    "                      analyzer='word',\n",
    "                      token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1, 3), \n",
    "                      use_idf=1,\n",
    "                      smooth_idf=1,\n",
    "                      sublinear_tf=1,\n",
    "                      stop_words = 'english',\n",
    "                     )\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "ypred = clf.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Logloss\n",
    "\n",
    "For a binary classification like our example, the typical loss function is the binary cross-entropy / log loss.\n",
    "\n",
    "$$H_p(q) = - \\frac{1}{N} \\sum_{i=1}^{N} y_i \\times log(p(y_i)) + (1-y_i) \\times log(1-p(y_i))$$\n",
    "\n",
    "Neural networks are trained using stochastic gradient descent and require that you choose a loss function when designing and configuring your model.\n",
    "\n",
    "Given input, the model is trying to make predictions that match the data distribution of the target variable. Under maximum likelihood, a loss function estimates how closely the distribution of predictions made by a model matches the distribution of target variables in the training data.\n",
    "\n",
    "**Cross-Entropy Loss** (or Log Loss)\n",
    "\n",
    "Cross-entropy loss is often simply referred to as “cross-entropy,” “logarithmic loss,” “logistic loss,” or “log loss” for short.\n",
    "\n",
    "Each predicted probability is compared to the actual class output value (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected value. The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score for a large difference (0.9 or 1.0).\n",
    "\n",
    "Cross-entropy loss is minimized, where smaller values represent a better model than larger values. A model that predicts perfect probabilities has a cross entropy or log loss of 0.0.\n",
    "\n",
    "Cross-entropy for a binary or two class prediction problem is actually calculated as the average cross entropy across all examples.\n",
    "\n",
    "Given a framework of maximum likelihood, we know that we want to use a cross-entropy or mean squared error loss function under stochastic gradient descent.\n",
    "\n",
    "Nevertheless, we may or may not want to report the performance of the model using the loss function.\n",
    "\n",
    "For example, logarithmic loss is challenging to interpret, especially for non-machine learning practitioner stakeholders. The same can be said for the mean squared error. Instead, it may be more important to report the accuracy and root mean squared error for models used for classification and regression respectively.\n",
    "\n",
    "It may also be desirable to choose models based on these metrics instead of loss. This is an important consideration, as the model with the minimum loss may not be the model with best metric that is important to project stakeholders.\n",
    "\n",
    "A good division to consider is to use the loss to evaluate and diagnose how well the model is learning. This includes all of the considerations of the optimization process, such as overfitting, underfitting, and convergence. An alternate metric can then be chosen that has meaning to the project stakeholders to both evaluate model performance and perform model selection.\n",
    "\n",
    "The same metric can be used for both concerns but it is more likely that the concerns of the optimization process will differ from the goals of the project and different scores will be required. Nevertheless, it is often the case that improving the loss improves or, at worst, has no effect on the metric of interest.\n",
    "\n",
    "\n",
    "**Reference**\n",
    "\n",
    "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "https://medium.com/30-days-of-machine-learning/day-5-entropy-relative-entropy-and-cross-entropy-8369d67cc180\n",
    "\n",
    "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Precision Recall curve\n",
    "\n",
    "    - Precision-Recall curves summarize the trade-off between the **true positive rate** and the **positive predictive value** for a predictive model using different probability thresholds.\n",
    "    - Precision-Recall curves are appropriate for imbalanced datasets, whereas ROC curves are appropriate when the observations are balanced between each class.\n",
    "\n",
    "**Precision** is a ratio of the number of true positives divided by the sum of the true positives and false positives. It describes how good a model is at predicting the positive class. Precision is referred to as the positive predictive value.\n",
    "\n",
    "$${Precision} = \\frac{True Positives} {(True Positives + False Positives)}$$\n",
    "\n",
    "**Recall** is calculated as the ratio of the number of true positives divided by the sum of the true positives and the false negatives. Recall is the same as sensitivity.\n",
    "\n",
    "$${Recall} = \\frac{True Positives} {(True Positives + False Negatives)}$$\n",
    "\n",
    "Reviewing both precision and recall is useful in cases where there is an imbalance in the observations between the two classes. Specifically, there are many examples of no event (class 0) and only a few examples of an event (class 1).\n",
    "\n",
    "The reason for this is that typically the large number of class 0 examples means we are less interested in the skill of the model at predicting class 0 correctly, e.g. high true negatives.\n",
    "\n",
    "There are also composite scores that attempt to summarize the precision and recall; three examples include:\n",
    "\n",
    "- **F score** or **F1 score**: that calculates the harmonic mean of the precision and recall (harmonic mean because the precision and recall are ratios).\n",
    "\n",
    "- **Average precision**: that summarizes the weighted increase in precision with each change in recall for the thresholds in the precision-recall curve.\n",
    "\n",
    "- **Area Under Curve**: like the AUC, summarizes the integral or an approximation of the area under the precision-recall curve.\n",
    "\n",
    "In terms of model selection, F1 summarizes model skill for a specific probability threshold, whereas average precision and area under curve summarize the skill of a model across thresholds, like ROC AUC.\n",
    "\n",
    "This makes precision-recall and a plot of precision vs. recall and summary measures useful tools for binary classification problems that have an imbalance in the observations for each class.\n",
    "\n",
    "The above explaination of Precision and Recall is taken from\n",
    "\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='logistic regression', features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### ROC curve\n",
    "\n",
    "    - ROC Curves summarize the trade-off between the **true positive rate** and **false positive rate** for a predictive model using different probability thresholds.\n",
    "    \n",
    "    - ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\n",
    "    \n",
    "$${True Positive Rate} = \\frac{True Positives} {(True Positives + False Negatives)}$$\n",
    "The true positive rate is also referred to as sensitivity.\n",
    "\n",
    "$${False Positive Rate} = \\frac{False Positives} {(False Positives + True Negatives)}$$\n",
    "\n",
    "The false positive rate is also referred to as the inverted specificity where specificity is the total number of true negatives divided by the sum of the number of true negatives and false positives.\n",
    "\n",
    "$${Specificity} = \\frac{True Negatives} {(True Negatives + False Positives)}$$\n",
    "\n",
    "$${False Positive Rate} = 1 - {Specificity}$$\n",
    "\n",
    "The ROC curve is a useful tool for a few reasons:\n",
    "\n",
    "- The curves of different models can be compared directly in general or for different thresholds.\n",
    "- The area under the curve (AUC) can be used as a summary of the model skill.\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='logistic regression',  features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_error_pred(yvalid, ypred):\n",
    "    \"\"\"\n",
    "    Generate a dataframe contains the missclassified data\n",
    "    \"\"\"\n",
    "    index_error = yvalid != ypred\n",
    "    x_error = xvalid[index_error]\n",
    "    ytrue_error = yvalid[index_error].values\n",
    "    ypred_error = ypred[index_error]\n",
    "    \n",
    "    df_error = pd.DataFrame({'x':x_error, 'y_true':ytrue_error, 'y_pred':ypred_error})\n",
    "    \n",
    "    return df_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_error = get_error_pred(yvalid, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_error.iterrows():\n",
    "    print('index: {}'.format(index))\n",
    "    print(row['x'])\n",
    "    print('true label: {}'.format(row['y_true']))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review 2 is not clear for me it was positive or negative.\n",
    "\n",
    "Reviews 4, 5 has some pros and cons.\n",
    "\n",
    "Reviews 3, 6 could be neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Count Features\n",
    "\n",
    "Instead of using TF-IDF, we can also use word counts as features. This can be done easily using CountVectorizer from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',\n",
    "                      token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1, 3), \n",
    "                      stop_words = 'english'\n",
    "                     )\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "ypred = clf.predict(xvalid_ctv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='logistic regression', features='count features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='logistic regression',  features='count features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "ypred = clf.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='Naive Bayes', features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='Naive Bayes',  features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that Navis bayes classifier with count features achieves better ROC AUC score (0.87) compare to other settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple Naive Bayes on Counts\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "ypred = clf.predict(xvalid_ctv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='Naive Bayes', features='count features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='Naive Bayes',  features='count features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "ypred = clf.predict(xvalid_tfv.tocsc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='XGBoost', features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='XGBoost',  features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on count features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "ypred = clf.predict(xvalid_ctv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='XGBoost', features='count features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='XGBoost',  features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that with xgboost on the tfidf, we achieve better roc-auc score compared to count features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier()\n",
    "\n",
    "param_dist = {'learning_rate': [0.01, 0.05, 0.07], \n",
    "              'max_depth': [4, 6, 8],\n",
    "              'n_estimators': [100, 200, 400],\n",
    "              #'min_child_weight': [2, 4],\n",
    "              #'silent': [1],\n",
    "              #'subsample': [0.7],\n",
    "              #'colsample_bytree': [0.7],\n",
    "              #'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "             }\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "# mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)\n",
    "search = RandomizedSearchCV(estimator = xgb_clf, \n",
    "                            param_distributions = param_dist, \n",
    "                            n_jobs = 4,\n",
    "                            verbose = 1,\n",
    "                            cv = 3,\n",
    "                            n_iter=20,\n",
    "                            #scoring=mll_scorer,\n",
    "                           )\n",
    "\n",
    "search.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "\n",
    "print('xgboost (hyperparameter tuning with grid search)')\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)\n",
    "\n",
    "predictions = search.predict_proba(xvalid_tfv.tocsc())\n",
    "ypred = search.predict(xvalid_tfv.tocsc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='XGBoost (RandomSearch)', features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='XGBoost (RandomSearch)',  features='TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that random search hyperparameter tuning does not improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Word Embeddings \n",
    "\n",
    "Download the GloVe vectors from here \n",
    "http://www-nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "\n",
    "\n",
    "**Reference**\n",
    "\n",
    "- Word embedding\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=5PL0TmQhItY\n",
    "\n",
    "\n",
    "\n",
    "- Glove embedding\n",
    "\n",
    "https://www.kaggle.com/stacykurnikova/using-glove-embedding\n",
    "\n",
    "https://www.quora.com/How-is-GloVe-different-from-word2vec\n",
    "\n",
    "\n",
    "- Fast word embedding\n",
    "\n",
    "https://www.kaggle.com/vsmolyakov/keras-cnn-with-fasttext-embeddings\n",
    "\n",
    "https://spenai.org/bravepineapple/faster_em/\n",
    "\n",
    "\n",
    "- NLP\n",
    "\n",
    "https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-069\n",
    "\n",
    "https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    #stop_words = nltk.download('stopwords')\n",
    "    #words = str(s).lower().decode('utf-8')\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "ypred = clf.predict(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='XGBoost', features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='XGBoost',  features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that using xgboost with glove embedding features does not improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvalid_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=xtrain_glove.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=10, verbose=1, \n",
    "          #validation_data=(xvalid_glove_scl, yvalid_enc)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(xvalid_glove_scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = predictions.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='deep learning', features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='deep learning',  features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- #### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = keras.preprocessing.text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, xtrain_glove.shape[1]))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     embedding_matrix.shape[1],\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(5, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=200, verbose=0, \n",
    "          callbacks=[earlystop],\n",
    "          #validation_data=(xvalid_pad, yvalid_enc)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(xvalid_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = predictions.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='lstm', features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='lstm',  features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     embedding_matrix.shape[1],\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(5, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=200, \n",
    "          verbose=0, \n",
    "          callbacks=[earlystop],\n",
    "          #validation_data=(xvalid_pad, yvalid_enc), \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(xvalid_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = predictions.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='gru', features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='gru',  features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- #### Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     embedding_matrix.shape[1],\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(5, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, \n",
    "          epochs=200, \n",
    "          verbose=0, \n",
    "          callbacks=[earlystop]\n",
    "          #validation_data=(xvalid_pad, yvalid_enc), \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(xvalid_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = predictions.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='bi-lstm', features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='bi-lstm',  features='glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Ensembling\n",
    "\n",
    "TODO: test the function\n",
    "\n",
    "https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"[%(asctime)s] %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\", stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Ensembler(object):\n",
    "    def __init__(self, model_dict, num_folds=3, task_type='classification', optimize=roc_auc_score,\n",
    "                 lower_is_better=False, save_path=None):\n",
    "        \"\"\"\n",
    "        Ensembler init function\n",
    "        :param model_dict: model dictionary, see README for its format\n",
    "        :param num_folds: the number of folds for ensembling\n",
    "        :param task_type: classification or regression\n",
    "        :param optimize: the function to optimize for, e.g. AUC, logloss, etc. Must have two arguments y_test and y_pred\n",
    "        :param lower_is_better: is lower value of optimization function better or higher\n",
    "        :param save_path: path to which model pickles will be dumped to along with generated predictions, or None\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_dict = model_dict\n",
    "        self.levels = len(self.model_dict)\n",
    "        self.num_folds = num_folds\n",
    "        self.task_type = task_type\n",
    "        self.optimize = optimize\n",
    "        self.lower_is_better = lower_is_better\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.training_data = None\n",
    "        self.test_data = None\n",
    "        self.y = None\n",
    "        self.lbl_enc = None\n",
    "        self.y_enc = None\n",
    "        self.train_prediction_dict = None\n",
    "        self.test_prediction_dict = None\n",
    "        self.num_classes = None\n",
    "\n",
    "    def fit(self, training_data, y, lentrain):\n",
    "        \"\"\"\n",
    "        :param training_data: training data in tabular format\n",
    "        :param y: binary, multi-class or regression\n",
    "        :return: chain of models to be used in prediction\n",
    "        \"\"\"\n",
    "\n",
    "        self.training_data = training_data\n",
    "        self.y = y\n",
    "\n",
    "        if self.task_type == 'classification':\n",
    "            self.num_classes = len(np.unique(self.y))\n",
    "            logger.info(\"Found %d classes\", self.num_classes)\n",
    "            self.lbl_enc = preprocessing.LabelEncoder()\n",
    "            self.y_enc = self.lbl_enc.fit_transform(self.y)\n",
    "            kf = StratifiedKFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, self.num_classes)\n",
    "        else:\n",
    "            self.num_classes = -1\n",
    "            self.y_enc = self.y\n",
    "            kf = KFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, 1)\n",
    "\n",
    "        self.train_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.train_prediction_dict[level] = np.zeros((train_prediction_shape[0],\n",
    "                                                          train_prediction_shape[1] * len(self.model_dict[level])))\n",
    "\n",
    "        for level in range(self.levels):\n",
    "\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                validation_scores = []\n",
    "                foldnum = 1\n",
    "                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n",
    "                    logger.info(\"Training Level %d Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if level != 0:\n",
    "                        l_training_data = temp_train[train_index]\n",
    "                        l_validation_data = temp_train[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    else:\n",
    "                        l0_training_data = temp_train[0][model_num]\n",
    "                        if type(l0_training_data) == list:\n",
    "                            l_training_data = [x[train_index] for x in l0_training_data]\n",
    "                            l_validation_data = [x[valid_index] for x in l0_training_data]\n",
    "                        else:\n",
    "                            l_training_data = l0_training_data[train_index]\n",
    "                            l_validation_data = l0_training_data[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "\n",
    "                    logger.info(\"Predicting Level %d. Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if self.task_type == 'classification':\n",
    "                        temp_train_predictions = model.predict_proba(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index,\n",
    "                        (model_num * self.num_classes):(model_num * self.num_classes) +\n",
    "                                                       self.num_classes] = temp_train_predictions\n",
    "\n",
    "                    else:\n",
    "                        temp_train_predictions = model.predict(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n",
    "                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n",
    "                    validation_scores.append(validation_score)\n",
    "                    logger.info(\"Level %d. Fold # %d. Model # %d. Validation Score = %f\", level, foldnum, model_num,\n",
    "                                validation_score)\n",
    "                    foldnum += 1\n",
    "                avg_score = np.mean(validation_scores)\n",
    "                std_score = np.std(validation_scores)\n",
    "                logger.info(\"Level %d. Model # %d. Mean Score = %f. Std Dev = %f\", level, model_num,\n",
    "                            avg_score, std_score)\n",
    "\n",
    "            logger.info(\"Saving predictions for level # %d\", level)\n",
    "            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n",
    "            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                        index=False, header=None)\n",
    "\n",
    "        return self.train_prediction_dict\n",
    "\n",
    "    def predict(self, test_data, lentest):\n",
    "        self.test_data = test_data\n",
    "        if self.task_type == 'classification':\n",
    "            test_prediction_shape = (lentest, self.num_classes)\n",
    "        else:\n",
    "            test_prediction_shape = (lentest, 1)\n",
    "\n",
    "        self.test_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.test_prediction_dict[level] = np.zeros((test_prediction_shape[0],\n",
    "                                                         test_prediction_shape[1] * len(self.model_dict[level])))\n",
    "        self.test_data = test_data\n",
    "        for level in range(self.levels):\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "                temp_test = self.test_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "                temp_test = self.test_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "\n",
    "                logger.info(\"Training Fulldata Level %d. Model # %d\", level, model_num)\n",
    "                if level == 0:\n",
    "                    model.fit(temp_train[0][model_num], self.y_enc)\n",
    "                else:\n",
    "                    model.fit(temp_train, self.y_enc)\n",
    "\n",
    "                logger.info(\"Predicting Test Level %d. Model # %d\", level, model_num)\n",
    "\n",
    "                if self.task_type == 'classification':\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test)\n",
    "                    self.test_prediction_dict[level][:, (model_num * self.num_classes): (model_num * self.num_classes) +\n",
    "                                                                                        self.num_classes] = temp_test_predictions\n",
    "\n",
    "                else:\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict(temp_test)\n",
    "                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n",
    "\n",
    "            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n",
    "            test_predictions_df.to_csv(os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                       index=False, header=None)\n",
    "\n",
    "        return self.test_prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the data to be used for every level of ensembling:\n",
    "train_data_dict = {0: [xtrain_tfv, xtrain_ctv, xtrain_tfv, xtrain_ctv], 1: [xtrain_glove]}\n",
    "test_data_dict = {0: [xvalid_tfv, xvalid_ctv, xvalid_tfv, xvalid_ctv], 1: [xvalid_glove]}\n",
    "\n",
    "model_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n",
    "\n",
    "              1: [xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}\n",
    "\n",
    "ens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n",
    "                optimize=multiclass_logloss, lower_is_better=True, save_path='')\n",
    "\n",
    "ens.fit(train_data_dict, ytrain, lentrain=xtrain_glove.shape[0])\n",
    "preds = ens.predict(test_data_dict, lentest=xvalid_glove.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_logloss(yvalid, preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(yvalid, predictions[:, 1], model_name='ensembling', features='tfidf, count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = predictions.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yvalid.values, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curve(yvalid, predictions[:, 1], model_name='ensembling',  features='tfidf, count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(yvalid, predictions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work\n",
    "\n",
    "- **Topic modeling**\n",
    "    - Have a deeper understanding about LDA\n",
    "\n",
    "- **Getting more data**\n",
    "    - Get review data from other flight companies\n",
    "    - Generate text data with neural networks\n",
    "\n",
    "- **Sentiment analysis with deep learning**\n",
    "    - Try different neural network architectures, e.g., BERT (https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d)\n",
    "\n",
    "- **Ensembling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Topic Modeling\n",
    "\n",
    "The following code is adapted from \n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m spacy download en\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Clean up the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df_review['Review'].values.tolist()\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Tokenize\n",
    "\n",
    "Tokenize and Clean-up using gensim’s simple_preprocess().\n",
    "\n",
    "Tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Lemmatization\n",
    "\n",
    "Lemmatization is a process where we convert words to its root word.\n",
    "\n",
    "For example: ‘Studying’ becomes ‘Study’, ‘Meeting becomes ‘Meet’, ‘Better’ and ‘Best’ becomes ‘Good’.\n",
    "\n",
    "The advantage of this is, we get to reduce the total number of unique words in the dictionary. \n",
    "\n",
    "As a result, the number of columns in the document-word matrix (created by CountVectorizer in the next step) will be denser with lesser columns.\n",
    "\n",
    "You can expect better topics to be generated in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' \n",
    "                                   for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Create the Document-Word matrix\n",
    "\n",
    "The LDA topic model algorithm requires a document word matrix as the main input.\n",
    "\n",
    "The below code configured the CountVectorizer to consider words that has occurred at least 10 times (min_df), \n",
    "remove built-in english stopwords, convert all words to lowercase, \n",
    "and a word can contain numbers and alphabets of at least length 3 in order to be qualified as a word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Check the Sparsicity\n",
    "\n",
    "Sparsicity is nothing but the percentage of non-zero datapoints in the document-word matrix, that is data_vectorized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Build LDA model with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_topics=5,                # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=42,           # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = 4,                # Use 4 available CPUs\n",
    "                                     )\n",
    "\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Diagnose model performance with perplexity and log-likelihood\n",
    "\n",
    "A model with higher log-likelihood and lower perplexity (exp(-1. * log-likelihood per word)) is considered to be good. Let’s check for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Use GridSearch to find the best LDA model?\n",
    "\n",
    "The most important tuning parameter for LDA models is n_components (number of topics). \n",
    "\n",
    "In addition, I am going to search learning_decay (which controls the learning rate) as well.\n",
    "\n",
    "Besides these, other possible search params could be learning_offset (downweigh early iterations. Should be > 1) and max_iter. These could be worth experimenting if you have enough computing resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [2, 4, 6, 8, 10], \n",
    "                 'learning_decay': [.5, .7, .9]\n",
    "                }\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Show the best topic model and its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that for LDA the best number of topics is 2. I believe the two topics would be positive review and negative review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Show the dominant topic in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Document - Topic Matrix\n",
    "#lda_output = lda_model.transform(data_vectorized)\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "# column names\n",
    "#topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_topics)]\n",
    "topicnames = [\"Topic\" + str(i) for i in range(2)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Review topics distribution across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Visualize the LDA model with pyLDAvis\n",
    "\n",
    "The pyLDAvis offers the best visualization to view the topics-keywords distribution.\n",
    "\n",
    "**TODO** : have a deep understanding of the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Show the Topic’s keywords?\n",
    "\n",
    "The weights of each keyword in each topic is contained in lda_model.components_ as a 2d array. The names of the keywords itself can be obtained from vectorizer object using get_feature_names().\n",
    "\n",
    "Let’s use this info to construct a weight matrix for all keywords in each topic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "\n",
    "# View\n",
    "print(df_topic_keywords.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Get the top 15 keywords each topic\n",
    "\n",
    "From the above output, I want to see the top 15 keywords that are representative of the topic.\n",
    "\n",
    "The show_topics() defined below creates that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### TODO: Cluster documents that share similar topics and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Reference\n",
    "\n",
    "**EDA**\n",
    "\n",
    "https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing\n",
    "\n",
    "https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n",
    "\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "https://medium.com/30-days-of-machine-learning/day-5-entropy-relative-entropy-and-cross-entropy-8369d67cc180\n",
    "\n",
    "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\n",
    "\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "\n",
    "\n",
    "**NLP**\n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/kernels\n",
    "\n",
    "https://www.kaggle.com/c/spooky-author-identification/kernels\n",
    "\n",
    "https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n",
    "\n",
    "https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing\n",
    "\n",
    "https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline\n",
    "\n",
    "https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n",
    "\n",
    "\n",
    "**General Data Science**\n",
    "\n",
    "https://www.kaggle.com/kernels?sortBy=voteCount&group=everyone&pageSize=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
